\section{Preprocessing}
\label{sec:03_Preprocessing}
Ziel des Preprocessing ist es die Daten aufzubereiten das der
Informationsgehalt maximiert sowie Rauschen minimiert wird.
Desweiteren muessen die Daten so vorverarbeitet werden, dass diese
diese von den machine learning algorithmen ausgewertet werden können. 

Um das Untegrundrauschen zu minimieren koennten Bildausschnitten auf 
denen keine Wolken zu erkennen sind herausgeschnitten werden.
Stattdessen wird auf die Methode der Farb-Filter zurueckgegriffen, da dabei 
kein manuelle Nachbearbeitung der Fotos notwendig ist und somit ein höherer 
Automatisierungsgrad erreicht wird.

Dazu werden Bilder, die weniger als \SI{30}{\percent} der maximalen Helligkeit 
besitzen, verworfen. 
Anders als Beispielsweise bei einer Zeitschaltuhr laesst sich durch die
Seperation anhand des Helligkeitswert die Messzeit maximieren, da die 
Belichtung vom Monat als auch von der Wolkendecke abhaengig ist.
Fotos welche nicht dem Helligkeitscut entprechen werden noch bevor sie 
klassifiziert werden der Klasse 'schlechte Fotos' zugeordent um den 
Arbeitsaufwand geringer zu halten.

\begin{wrapfigure}{r}{0.5\textwidth}
		\centering
		\includegraphics[width=0.49\textwidth]{pictures/cut_cube.pdf}
		\caption{Um den Winkel $\alpha$ zur Gruenebene geneigte Parabel im RGB 
				Farbraum, welche zu den Blauwerten geöffnet ist.}
				\label{fig:parabular}
\end{wrapfigure}
Das Wolkenspektrum hatt klar definierte Farben die Hauptsächlich aus Blau, Grau
und Weiß Tönen besteht.
Pixel die nicht zu diesem Spektrum gehörsn werden systematisch auf den
Minimalwert gesetzt.
Dazu wird der Farbraum in ein Rotiertes System $RGB'$ mittels der 
Rotationsmatrix $R_{\alpha}$ um den Nullpunkt gehdreht. 
Dem rotierten Farbraum $RGB'$ wird eine Parabel gelegt die Pixel verwirft, 
welche die Ungleichung 
\begin{equation}
		c' > (b' - x_0)^2 + x_1, \hspace{3em} c' \in (r', g')
\end{equation}
nicht erfüllen und anschließend die Parabel in den ungestrichenen Raum zurueck
rotiert.
Der Nullpunkt der Parabel wird mit den Helligkeitswerten verschoben.
Ein Beispiel ist fuer Alle  drei Kanäle in Abbildung \ref{fig:parabular} zu sehen.
Dadurch laesst sich ein grossteil der mitfotographierten Untergrunddaten durch
eine Konstanten Wert ersetzen \texttt{(0,0,0)}.
\begin{figure}
		\centering
		\includegraphics[width=0.95\textwidth]{pictures/cut_hist.pdf}
		\caption{Anhand des Farbspektrums geschwärzter Untergrund und die
		dazugehörigen Farbspektra zur Reduzierung des Umgebungsrauschens mit
		\texttt{bins = 10} zur Veranschauungszwecken.}
		\label{fig:name}
\end{figure}

Nachdem die Daten entsprechend aufbereitet wurden mussen sie noch in Form fuer
die Algorithmen gebracht werden. 
Dazu wird der Farbraum fuer den Random Forrest diskretisiert.
Durch mehrmaliges austesten kristallisierte sich eine Anzahl von 
\texttt{bins = 30} als die Diskretisierung mit den besten Ergebnissen heraus,
wobei die auf den Konstanten Wert gesetzten Untergrunddaten kein Teil des
Histogramms sind.
Fuer das Neuronale Netz werden die Bilddaten auf eins normiert und mittels 
eines \texttt{ImageDataGenerator} sequentiell aus den \texttt{JPG}-Dateien 
eingelesen. 
Dadurch laesst sich das Überlaufen des begrenzten Arbeitsspeicher auf Kosten 
der Trainingszeit, durch das wiederholte Laden der Daten von der Festplatte,
verhindern.


\section{Machine Learning}

Zur automatischen Bestimmung des Wolkentyps werden zwei verschiedene 
Algorithmen verwendet. 
Der Random Forest wird verwendet weil dieser out of the box hinreichend 
schnell, in der Auswertung und resourcendschonen ist.
Desweiteren wird ein CNN benutzt da dieses im Gegensatz zum Random Forest in 
der Laage ist sowohl auf den Wolkenformen sowie auch dem Farbspektrum zu
trainieren. 

Beim training der Algorithmen stellte sich heraus das die Daten aufgrund des im
Kapitel \ref{sec:02_Datensatz} beschriebene Problem ein grossen Missmatch 
aufweisen. 
Aufgrund dessen ändert sich die Zielstellung bei der Optimierung wesenltich.
Ziel ist vorerst nicht einen moeglichst hohe Genauigkeit zu erlangen um die
Wolkenklassifikation auf den PIs voran zu treiben, sondern nun den Datensatz zu
erweitern und den Missmatch zu minimieren.
Dazu werden die Methoden genutzt um die Wolken welche nicht mit dem aktuellen
Label übereinstimmen mittels dem Telegram Bot erneut zu überprüfen.
Desweiteren wird bei dem labeln neuer Daten immer ein Label vorgeschlagen
welches übernommen oder per Hand gelabelt werden kann.
\begin{figure}
		\centering
		\includegraphics[width=\textwidth]{build/vorgehen.pdf}
		\caption{Modell fuer weiteres Vorgehen zur verbesserung der Vorhersagen
		der maschinellen Modelle durch erstellung eines reineren Datensatzes.}
		\label{fig:}
\end{figure}

Die Metrik anhand derer die Modelle evaluiert werden bleibt die ACC wobei 
abgeleitete Größen wie der Loss oder confidence Werte, kritisch bei Daten 
welche einen Missmatch haben, zu betrachten sind.

\subsection{Random Forest}%
\label{sub:random_forest}
\begin{wrapfigure}{r}{0.5\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{./pictures/train_rf.pdf}
		\caption{Feature Importance des Random Forest.}
		\label{fig:}
\end{wrapfigure}
Fuer das Training des Random Forrest koennen mehrere Parameter variiert werden.
Neben der maximalen Tiefe, der Anzahl an gezogenen Feature pro Baum kann auch 
die Anzahl an Entscheifungsbaeumen varriert werden.
Da die Methode des Random Forest jedoch durch einien hohe Anzahl an Bäumen
gegen Overfitting geschuetzt werden, werden die Tiefe der Baeume nicht weiter
beschränkt und die Anzahl an gezogenen Featuren nicht weiter optimiert.
Desweiteren ist der histogrammierte Datensatz mit 30 bins pro Farbkanal für
machine learning algorithmen sehr niederdimensional.

\subsection{Convolution Neuronal Network}%
\label{sub:convolution_neuronal_network}

Die optimierung des Netzes steht unter der permisse die Architecture des Netzes
so einfach zu halten das die ACC maximal wird und die Parameteranzahl, welche 
mit der auswertungszeit korrellieren kann, gering bleibt. 
Beim Training mit der \texttt{categorical\_crossentropy} als Validation loss stellt sich 
wider erwarten heraus das der validation loss bei guten Vorhersagen bei einem
Datensatz mit einem Missmatch steigt. 
Dies liegt daran wenn zum Beispiel bei dem waren Label $A$ der Datensatz das label
$B$ hat. 
\begin{equation}
		H(p,q) = -\sum_x p(x) \log q(x)
\end{equation}
Somit wird die Wahrscheinlichkeit $q(B)$ die ein trainiertes Netz für die 
flasch klassifizierte Klasse klein und die Kreuzentropie groß.
Dies hat zur Folge das die Kreuzentropie bei Datensaetzen mit einem Missmatch
bei der Validierung fuer hohe ACC nicht abnimmt sondern schnell sehr groß wird.
Das Übertraining kann durch eine Lossfunktion welche nicht so sensible auf
Missmatches ist verringert werden. 
Dies ist beispielsweise fuer die in der Analyse verwendete \texttt{logcosh} der
Fall. 
Sie aehnlt fuer kleine Werte dem mittleren Quadratischen Fehler und nimmt für
große Werte einen linearen zusammenhang an.
Desweiteren wird im Rahmen der Moeglichkeiten versucht den Missmatch der Daten 
zu minimieren.

\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{pictures/architecture.pdf}
		\caption{}
		\label{fig:}
\end{figure}
Als architectur wird zunächst eine \texttt{AveragePooling2D} Schicht mit einer
\texttt{poolsize} von \texttt{(3,4)} gewählt um die Dimesonalität des Bildes zu verringern.
Dabei viel die wahl gegen die \texttt{MaxPooling2D} Schicht da durch das 
Mitteln der Pixel deren Rauschen verringert werden kann.
Anschließend folgen sechs \texttt{Conv2D} Schichten die der Faltung des Bildes
dienen. 
Die erste Convolutionschicht besitzt ein Schrittweite von \texttt{strides =
(3,3)} was ebenso der Dimesnionsreduktion dient.
Zwischen der Convolution zweiten und dritten Schicht wird mittels
\texttt{MaxPooling2d} die Dimesonalität weiter verringert. 
Im anschluss folgen nach der letzten \texttt{Conv2D} Schicht eine weitere
\texttt{MaxPooling2D} Schicht bevor die Daten geflattet werden.
Beim \texttt{Flatten} wird aus einem hochdimensionalen Vektor ein flacher Vektor
mit der gleichen Anzahl an Einträgen.
Dieser kann von zwei aufeinanderfolgenden \texttt{Dense} Schichten verarbeitet
werden.
\texttt{Dense} Schicht wird der Outpuvektor der Flattenschicht $x_i$ mit den
Gewichten $w_\text{ij}$ multipliziert, welche die Verbindung zur Folgeschicht
$j$ herstellen.
Diese werden durch jeweils einer \texttt{Dropout} und \texttt{GaussianNoise} 
Schicht regularisiert. 
Dabei dient die \texttt{Dropout}- dazu die Gewichte $w_\text{ij}$ ausgeglichen 
und die \texttt{Noise}-Schicht zu gewährleisten das diese klein sind.
Abschließend folgt eine Dense schicht mit der Anzahl an neuronen der
Zielklassen.
Fuer die Kernel sowie die Dens schichten wird die \texttt{relu} Funktion als
Aktivierungsfunktion genutzt.
Fuer die letzte Schicht wird entsprechend die \texttt{sigmoid} Funktion verwendet um die
Vorraussagen glatt zu machen und auf 1 zu normieren.
